---
id: 96
title: 海外推广-google HTML优化之删除重复的元
date: 2019-12-30T02:06:49+08:00
author: netrob
excerpt: google已经放弃head内关键词的支持，所以更说明加强了description的读取，而google HTML优化工具就是一个海外推广外贸建站必选seo工具了。
layout: post
guid: https://www.justcn.cn/?p=96
permalink: '/%e6%b5%b7%e5%a4%96%e6%8e%a8%e5%b9%bf-google-html%e4%bc%98%e5%8c%96%e4%b9%8b%e5%88%a0%e9%99%a4%e9%87%8d%e5%a4%8d%e7%9a%84%e5%85%83/'
categories:
  - 营销工具
tags:
  - robots
  - seo
  - 海外推广
---
google已经放弃head内关键词的支持，所以更说明加强了description的读取，而google HTML优化工具就是一个海外推广外贸建站必选seo工具了。

**HTML 优化**  
简单说就是检测网站内有哪些的页面description描述是一致的，就是谷歌说的元——它能让用户清楚地了解您网站的内容。修改或者删除重复的元。

**如何安全的创建robots.txt 文件**  
robots.txt 文件是所有站长都知道的文件，它泄漏了太多的目录信息和网站信息。那么google提倡使用robots.txt ，却没有过多提到它的危害性。我们需要使用robots.txt 语法创建一套安全快捷的命令。

## robots.txt 命令

最简单的robots.txt文件会用到两个关键字：User-agent和Disallow。User-agent（用户代理）是指搜索引擎漫游器（即网页抓取工具软件）；Web Robots Database 中列出了大多数用户代理。Disallow 是针对用户代理的命令，指示它不要访问某个特定网址。反之，如果要允许 Google 访问某个特定网址，而该网址是已禁止访问的父级目录中的子目录，则可以使用第三个关键字 Allow。  
使用上述关键字时应遵循以下语法：  
User-agent: [the name of the robot the following rule applies to]  
Disallow: [the URL path you want to block]  
Allow: [the URL path in of a subdirectory, within a blocked parent directory, that you want to unblock]  
这两行加起来被视为文件中的一个条目，其中的disallow规则仅适用于其上方指定的用户代理。您可以根据需要添加任意数量的条目，并且可以在一个条目中添加多个Disallow行，这些Disallow行可以应用到同一个条目中的多个用户代理。您可以将User-agent命令设置为应用到所有网页抓取工具（以星号(*)列出），如下所示：  
User-agent: *  
可在 robots.txt 文件中使用的网址拦截命令

禁止访问…  
示例  
禁止访问整个网站（使用正斜线(/)表示）： Disallow: /  
禁止访问某一目录以及其中的所有内容（在目录名后添加正斜线）： Disallow: /sample-directory/  
禁止访问某个网页（在正斜线后列出网页）： Disallow: /private_file.html  
禁止Google图片访问特定图片： User-agent: Googlebot-Image  
Disallow: /images/dogs.jpg  
禁止Google图片访问您网站上的所有图片： User-agent: Googlebot-Image  
Disallow: /  
禁止访问特定类型的文件（例如.gif）： User-agent: Googlebot  
Disallow: /*.gif$  
禁止访问您网站上的网页，但允许在这些网页上显示AdSense广告（禁止使用Mediapartners-Google以外的所有网页抓取工具）。这种方法会阻止您的网页显示在搜索结果中，但Mediapartners-Google网页抓取工具仍然可以分析这些网页，以确定向您网站的访问者显示哪些广告。 User-agent: *  
Disallow: /  
User-agent: Mediapartners-Google  
Allow: /  
请注意，指令区分大小写。例如，Disallow: /file.asp会禁止访问http://www.example.com/file.asp，但允许访问http://www.example.com/File.asp。Googlebot还会忽略 robots.txt中的空格和未知指令。  
用于简化robots.txt代码的格式匹配规则  
格式匹配规则  
示例  
要禁止访问任意字符序列，请使用星号(_)。例如，示例代码会禁止访问所有以“private”开头的子目录： User-agent: Googlebot Disallow: /private_/  
要禁止访问所有包含问号(?)的网址。例如，示例代码会禁止访问以您的域名开头、后接任意字符串、然后接问号、而后又接任意字符串的网址： User-agent: Googlebot  
Disallow: /_? 要禁止访问以某种特定方式结尾的所有网址，请使用$。例如，示例代码会禁止访问以.xls结尾的所有网址： User-agent: Googlebot Disallow: /_.xls$  
要组合使用Allow和Disallow指令禁止访问某些格式的网址，请查看右侧的示例。在此示例中，? 表示会话 ID。您通常需要阻止 Google 访问包含这些 ID 的网址，以防止网页抓取工具抓取重复的网页。不过，如果某些以?结尾的网址 是您想要抓取的网页的不同版本，那么您可以使用下面这种组合Allow和Disallow指令的方法：  
Allow: /*?$指令将允许访问以?结尾的所有网址（更确切地说，该指令将允许访问以您的域名开头、后接任意字符串、然后接?、而后不再接任何字符的网址）。  
Disallow: / *? 指令将禁止访问包含?的所有网址 （更确切地说，该指令将禁止访问以您的域名开头、后接任意字符串、然后接问号、而后又接任意字符串的网址）。  
User-agent: *  
Allow: /_?$ Disallow: /_?

## robots.txt测试工具

这个是必须使用的工具，它将确保你的robots.txt文件正确和有效。每次更新robots.txt都需要重新提交以确保robots.txt的有效性。同时可以网页是否被拦截而影响seo效果。  
google robots.txt测试工具  
值得一提的是，通配符将大大提升安全性和效率。再测试时请注意不要在目录前加 / 这个字符。